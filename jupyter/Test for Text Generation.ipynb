{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TEXTMORPH_DATA=/Users/guillaume/Desktop/backup/neural-editor-grll-data\n"
     ]
    }
   ],
   "source": [
    "%env TEXTMORPH_DATA=/Users/guillaume/Desktop/backup/neural-editor-grll-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, realpath, join\n",
    "\n",
    "from textmorph.text_generation.training_run import EditTrainingRuns\n",
    "from textmorph import data\n",
    "from textmorph.text_generation.training_run import GenData\n",
    "from gtd.ml.torch.utils import similar_size_batches\n",
    "from gtd.chrono import verboserate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = join(data.workspace.root, \"textgen_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[localhost] local: wc -l /Users/guillaume/Desktop/backup/neural-editor-grll-data/textgen_dataset/IVR_text_HiGe_SCinternal\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff52177ec384be8865dfdb808ebaa2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Reading data file.', max=208), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "source_data = GenData(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded checkpoint #100000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02db30f9221f4ea6ae3522f5f75ab807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SEJveChjaGlsZHJlbj0oSW50UHJvZ3Jlc3ModmFsdWU9MCwgZGVzY3JpcHRpb249dSdMb2FkaW5nIGVtYmVkZGluZ3MgZnJvbSAvVXNlcnMvZ3VpbGxhdW1lL0Rlc2t0b3AvYmFja3VwL25ldXLigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No GPUs detected. Sticking with CPUs.\n"
     ]
    }
   ],
   "source": [
    "experiments = EditTrainingRuns(check_commit=False)\n",
    "exp_id = [6]\n",
    "exp = experiments[int(exp_id[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Editor(\n",
       "  (encoder): Encoder(\n",
       "    (token_embedder): TokenEmbedder(\n",
       "      (_embedding): TrainFlagEmbedding(\n",
       "        (_embedding): Embedding(20003, 300)\n",
       "      )\n",
       "    )\n",
       "    (source_encoder): MultiLayerSourceEncoder(\n",
       "      (encoder_layer_0): BidirectionalSourceEncoder(\n",
       "        (forward_encoder): SimpleSourceEncoder(\n",
       "          (rnn_cell): LSTMCell(300, 128)\n",
       "        )\n",
       "        (backward_encoder): SimpleSourceEncoder(\n",
       "          (rnn_cell): LSTMCell(300, 128)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): BidirectionalSourceEncoder(\n",
       "        (forward_encoder): SimpleSourceEncoder(\n",
       "          (rnn_cell): LSTMCell(256, 128)\n",
       "        )\n",
       "        (backward_encoder): SimpleSourceEncoder(\n",
       "          (rnn_cell): LSTMCell(256, 128)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): BidirectionalSourceEncoder(\n",
       "        (forward_encoder): SimpleSourceEncoder(\n",
       "          (rnn_cell): LSTMCell(256, 128)\n",
       "        )\n",
       "        (backward_encoder): SimpleSourceEncoder(\n",
       "          (rnn_cell): LSTMCell(256, 128)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (edit_encoder): EditEncoder(\n",
       "      (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear_prenoise): Linear(in_features=300, out_features=64, bias=False)\n",
       "      (normclip): Hardtanh(min_val=0, max_val=13.9)\n",
       "    )\n",
       "    (agenda_maker): AgendaMaker(\n",
       "      (linear): Linear(in_features=384, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (train_decoder): TrainDecoder(\n",
       "    (decoder_cell): AttentionDecoderCell(\n",
       "      (decoder_layer_0): LSTMCell(1412, 256)\n",
       "      (decoder_layer_1): LSTMCell(1368, 256)\n",
       "      (decoder_layer_2): LSTMCell(1368, 256)\n",
       "      (vocab_projection_pos): Linear(in_features=1112, out_features=300, bias=True)\n",
       "      (vocab_projection_neg): Linear(in_features=1112, out_features=300, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (vocab_softmax): Softmax()\n",
       "      (source_attention): Attention(\n",
       "        (tanh): Tanh()\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (insert_attention): Attention(\n",
       "        (tanh): Tanh()\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (delete_attention): Attention(\n",
       "        (tanh): Tanh()\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (token_embedder): TokenEmbedder(\n",
       "        (_embedding): TrainFlagEmbedding(\n",
       "          (_embedding): Embedding(20003, 300)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (token_embedder): TokenEmbedder(\n",
       "      (_embedding): TrainFlagEmbedding(\n",
       "        (_embedding): Embedding(20003, 300)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized batches: reduced cost from 1568 (naive) to 1568 (0.0% reduction).\n",
      "Optimal (batch_size=1) would be 1456.\n"
     ]
    }
   ],
   "source": [
    "batches = similar_size_batches(source_data.data, 32, size=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e230e6da0bb54e868da07f29d8f13901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Streaming Source Sentences', max=7), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in verboserate(batches, desc='Streaming Source Sentences'):\n",
    "    # Source Encode\n",
    "    source_words, insert_words, insert_exact_words, delete_words, delete_exact_words, target_words, edit_embed = exp.editor._batch_editor_examples(batch)\n",
    "    encoder_input = exp.editor.encoder.preprocess(source_words, insert_words, insert_exact_words, delete_words, delete_exact_words, edit_embed)\n",
    "    encoder_output = exp.editor.encoder(encoder_input)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderOutput(source_embeds=SequenceBatch(values=tensor([[[ 0.0105, -0.0172,  0.0120,  ..., -0.0238,  0.0066, -0.0048],\n",
       "         [-0.0010, -0.2149,  0.0760,  ..., -0.0335,  0.0214,  0.0278],\n",
       "         [-0.0102,  0.3015,  0.0230,  ..., -0.0974,  0.0172,  0.0054],\n",
       "         ...,\n",
       "         [-0.0095, -0.0314, -0.0167,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [-0.0095, -0.0314, -0.0167,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [-0.0095, -0.0314, -0.0167,  ..., -0.1196,  0.1468, -0.1008]],\n",
       "\n",
       "        [[ 0.0102, -0.0176,  0.0121,  ..., -0.0676,  0.0052, -0.0025],\n",
       "         [-0.0011, -0.2028,  0.0799,  ..., -0.0741,  0.0168,  0.0328],\n",
       "         [-0.0104,  0.2999,  0.0268,  ..., -0.1959,  0.0155,  0.0107],\n",
       "         ...,\n",
       "         [-0.1257, -0.0476,  0.4313,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [-0.1257, -0.0476,  0.4313,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [-0.1257, -0.0476,  0.4313,  ..., -0.1196,  0.1468, -0.1008]],\n",
       "\n",
       "        [[ 0.0102, -0.0176,  0.0121,  ..., -0.0676,  0.0052, -0.0025],\n",
       "         [-0.0011, -0.2028,  0.0799,  ..., -0.0741,  0.0168,  0.0328],\n",
       "         [-0.0104,  0.2999,  0.0268,  ..., -0.1959,  0.0155,  0.0107],\n",
       "         ...,\n",
       "         [-0.1257, -0.0476,  0.4313,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [-0.1257, -0.0476,  0.4313,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [-0.1257, -0.0476,  0.4313,  ..., -0.1196,  0.1468, -0.1008]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0168, -0.0059,  0.0173,  ...,  0.0428, -0.0480, -0.0721],\n",
       "         [ 0.1255,  0.1574,  0.2872,  ...,  0.0349, -0.0620, -0.1794],\n",
       "         [ 0.3747,  0.0155, -0.0548,  ...,  0.0118, -0.0132, -0.0207],\n",
       "         ...,\n",
       "         [-0.1457,  0.0206,  0.0294,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [-0.1457,  0.0206,  0.0294,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [-0.1457,  0.0206,  0.0294,  ..., -0.1196,  0.1468, -0.1008]],\n",
       "\n",
       "        [[ 0.0130, -0.0173,  0.0132,  ...,  0.0206,  0.0102, -0.0169],\n",
       "         [-0.0004, -0.2721,  0.0932,  ...,  0.0098,  0.0253, -0.0277],\n",
       "         [-0.0110,  0.3059,  0.0321,  ...,  0.0128, -0.0049, -0.0508],\n",
       "         ...,\n",
       "         [-0.0042, -0.0685, -0.0103,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [-0.0042, -0.0685, -0.0103,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [-0.0042, -0.0685, -0.0103,  ..., -0.1196,  0.1468, -0.1008]],\n",
       "\n",
       "        [[ 0.0113, -0.0193,  0.0126,  ..., -0.0004,  0.0035, -0.0127],\n",
       "         [-0.1279, -0.1006,  0.0585,  ..., -0.0051,  0.2315, -0.1584],\n",
       "         [-0.0429,  0.2503, -0.0417,  ..., -0.0208, -0.0269, -0.0204],\n",
       "         ...,\n",
       "         [ 0.3717, -0.0301, -0.0081,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [ 0.3717, -0.0301, -0.0081,  ..., -0.1196,  0.1468, -0.1008],\n",
       "         [ 0.3717, -0.0301, -0.0081,  ..., -0.1196,  0.1468, -0.1008]]],\n",
       "       grad_fn=<CatBackward>), mask=tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])), insert_embeds=SequenceBatch(values=tensor([[[-0., -0., -0.,  ..., -0.,  0., -0.]],\n",
       "\n",
       "        [[ 0., -0., -0.,  ..., -0., -0.,  0.]],\n",
       "\n",
       "        [[ 0., -0., -0.,  ..., -0., -0.,  0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0., -0., -0.,  ..., -0.,  0.,  0.]],\n",
       "\n",
       "        [[-0., -0., -0.,  ..., -0.,  0., -0.]],\n",
       "\n",
       "        [[ 0., -0., -0.,  ..., -0.,  0., -0.]]], grad_fn=<MulBackward>), mask=tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])), delete_embeds=SequenceBatch(values=tensor([[[-0.,  0., -0.,  ..., -0., -0.,  0.]],\n",
       "\n",
       "        [[-0., -0.,  0.,  ...,  0., -0., -0.]],\n",
       "\n",
       "        [[-0., -0.,  0.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0., -0., -0.,  ...,  0., -0.,  0.]],\n",
       "\n",
       "        [[-0.,  0.,  0.,  ..., -0.,  0., -0.]],\n",
       "\n",
       "        [[-0., -0.,  0.,  ..., -0., -0.,  0.]]], grad_fn=<MulBackward>), mask=tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])), agenda=tensor([[-0.1616,  0.1792, -0.0777,  ..., -0.0553, -0.0119,  0.2635],\n",
       "        [-0.2865,  0.1827,  0.0172,  ..., -0.2316, -0.1600,  0.1097],\n",
       "        [-0.2865,  0.1827,  0.0172,  ..., -0.2316, -0.1600,  0.1097],\n",
       "        ...,\n",
       "        [-0.2506, -0.8162,  0.8813,  ...,  0.2443,  0.9135, -0.4529],\n",
       "        [-0.2017, -1.4698,  0.7047,  ...,  0.3952,  0.6425,  0.4946],\n",
       "        [-0.4426, -0.2173,  0.4433,  ...,  0.0091, -0.1476,  0.2724]],\n",
       "       grad_fn=<ThAddmmBackward>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
